// Generated by Copilot: Implementing TensorZero handler functionality
#include "tensorzero_handler.hh"
#include "src/utils/logging.hh"
#include <memory>

namespace torchserve
{

  void TensorZeroHandler::initialize(const std::string &model_path)
  {
    try
    {
      model_ = std::make_unique<tensorzero::Model>();
      auto status = model_->Load(model_path);
      if (!status.ok())
      {
        throw std::runtime_error("Failed to load TensorZero model: " + status.error_message());
      }
      LOG_INFO("TensorZero model loaded successfully from: %s", model_path.c_str());
    }
    catch (const std::exception &e)
    {
      LOG_ERROR("Error initializing TensorZero model: %s", e.what());
      throw;
    }
  }

  void TensorZeroHandler::preprocess(const RequestContext &context)
  {
    try
    {
      // Validate input format
      if (context.request_input().empty())
      {
        throw std::runtime_error("Empty request input");
      }
      LOG_DEBUG("TensorZero preprocessing completed for request");
    }
    catch (const std::exception &e)
    {
      LOG_ERROR("Error in TensorZero preprocessing: %s", e.what());
      throw;
    }
  }

  ts_error_message TensorZeroHandler::inference(const RequestContext &context, ResponseContext &response)
  {
    if (!model_)
    {
      return "Model not initialized";
    }

    try
    {
      tensorzero::InputData input(context.request_input());
      auto result = model_->Run(input);
      if (!result.ok())
      {
        LOG_ERROR("TensorZero inference failed: %s", result.error_message().c_str());
        return "Inference failed: " + result.error_message();
      }

      response.set_response_data(result.value());
      LOG_DEBUG("TensorZero inference completed successfully");
      return "";
    }
    catch (const std::exception &e)
    {
      LOG_ERROR("Exception during TensorZero inference: %s", e.what());
      return std::string("Exception during inference: ") + e.what();
    }
  }

  void TensorZeroHandler::postprocess(ResponseContext &response)
  {
    try
    {
      // Add any necessary response formatting here
      LOG_DEBUG("TensorZero postprocessing completed");
    }
    catch (const std::exception &e)
    {
      LOG_ERROR("Error in TensorZero postprocessing: %s", e.what());
      throw;
    }
  }

} // namespace torchserve